{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/oai_client_cost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Usage tracking with AtuoGen\n",
    "## 1. Use AutoGen's OpenAIWrapper for cost estimation\n",
    "The `OpenAIWrapper` from `autogen` tracks token counts and costs of your API calls. Use the `create()` method to initiate requests and `print_usage_summary()` to retrieve a detailed usage report, including total cost and token usage for both cached and actual requests.\n",
    "\n",
    "- `mode=[\"actual\", \"total\"]` (default): print usage summary for non-caching completions and all completions (including cache).\n",
    "- `mode='actual'`: only print non-cached usage.\n",
    "- `mode='total'`: only print all usage (including cache).\n",
    "\n",
    "Reset your session's usage data with `clear_usage_summary()` when needed.\n",
    "\n",
    "## 2. Track cost and token count for agents\n",
    "We also support cost estimation for agents. Use `Agent.print_usage_summary()` to print the cost summary for the agent.\n",
    "You can retrieve usage summary in a dict using `Agent.get_actual_usage()` and `Agent.get_total_usage()`. Note that `Agent.reset()` will also reset the usage summary.\n",
    "\n",
    "To gather usage data for a list of agents, we provide an utility function `autogen.agent_utils.gather_usage_summary(agents)` where you pass in a list of agents and gather the usage summary.\n",
    "\n",
    "## Caution when using Azure OpenAI!\n",
    "If you are using azure OpenAI, the model returned from completion doesn't have the version information. The returned model is either 'gpt-35-turbo' or 'gpt-4'. From there, we are calculating the cost based on gpt-3.5-0613: ((0.0015, 0.002) per 1k prompt and completion tokens) and gpt-4-0613: (0.03,0.06). This means the cost is wrong if you are using the 1106 version of the models from azure OpenAI.\n",
    "\n",
    "This will be improved in the future. However, the token count summary is accurate. You can use the token count to calculate the cost yourself.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`:\n",
    "```bash\n",
    "pip install \"pyautogen\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen\n",
      "  Downloading pyautogen-0.2.10-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m14.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting diskcache\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting docker\n",
      "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m17.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flaml\n",
      "  Downloading FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m22.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openai>=1.3 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from pyautogen) (1.3.7)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from pyautogen) (1.10.10)\n",
      "Requirement already satisfied: python-dotenv in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from pyautogen) (1.0.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: tiktoken in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from pyautogen) (0.5.1)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (0.24.1)\n",
      "Requirement already satisfied: sniffio in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from openai>=1.3->pyautogen) (4.5.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from docker->pyautogen) (23.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from docker->pyautogen) (2.29.0)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m13.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: NumPy>=1.17.0rc1 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from flaml->pyautogen) (1.24.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from tiktoken->pyautogen) (2022.7.9)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai>=1.3->pyautogen) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2023.5.7)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.17.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from requests>=2.26.0->docker->pyautogen) (2.0.4)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
      "Installing collected packages: urllib3, termcolor, flaml, diskcache, docker, pyautogen\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed diskcache-5.6.3 docker-7.0.0 flaml-2.1.1 pyautogen-0.2.10 termcolor-2.4.0 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.10.1\n",
      "Uninstalling scipy-1.10.1:\n",
      "  Would remove:\n",
      "    /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages/scipy\n",
      "    /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages/scipy-1.10.1-py3.11.egg-info\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: scipy in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/lg/miniconda3/envs/py311/lib/python3.11/site-packages (from scipy) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall scipy\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import OpenAIWrapper\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from autogen.agent_utils import gather_usage_summary\n",
    "\n",
    "# config_list = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": [\"gpt-3.5-turbo\", \"gpt-4-1106-preview\"],\n",
    "#     },\n",
    "# )\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-1106\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well).\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"<your OpenAI API key>\",\n",
    "    },  # OpenAI API endpoint for gpt-4\n",
    "    {\n",
    "        \"model\": \"gpt-35-turbo-0613\",  # 0613 or newer is needed to use functions\n",
    "        \"base_url\": \"<your Azure OpenAI API base>\", \n",
    "        \"api_type\": \"azure\", \n",
    "        \"api_version\": \"2023-08-01-preview\", # 2023-07-01-preview or newer is needed to use functions\n",
    "        \"api_key\": \"<your Azure OpenAI API key>\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/website/docs/llm_endpoint_configuration.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAIWrapper with cost estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8o3WMtChih6VZFS0BeaQC1LqhKAEE', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='1. Practice regularly: Consistent practice is key to learning Python effectively. Set aside time each day or week to work on coding exercises or projects to reinforce your understanding of Python concepts.\\n\\n2. Utilize online resources: Take advantage of the abundance of online tutorials, coding exercises, and forums available for Python learners. Sites like Codecademy, Coursera, or Python.org offer free or affordable resources to help you master Python.\\n\\n3. Break down problems into smaller tasks: When faced with a coding challenge, break it down into smaller, manageable tasks. Tackle each task step by step, ensuring that you understand and test each component before moving on. Breaking down problems will help you approach complex coding challenges with clarity and efficiency.', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1706940626, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=147, prompt_tokens=25, total_tokens=172), cost=0.0003315, message_retrieval_function=<bound method OpenAIClient.message_retrieval of <autogen.oai.client.OpenAIClient object at 0x136368d50>>, config_id=0, pass_filter=True)\n",
      "0.0003315\n"
     ]
    }
   ],
   "source": [
    "client = OpenAIWrapper(config_list=config_list)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you give me 3 useful tips on learning Python? Keep it simple and short.\"},\n",
    "]\n",
    "response = client.create(messages=messages, model=\"gpt-3.5-turbo\", cache_seed=None)\n",
    "print(response)\n",
    "print(response.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Summary for OpenAIWrapper\n",
    "\n",
    "When creating a instance of OpenAIWrapper, cost of all completions from the same instance is recorded. You can call `print_usage_summary()` to checkout your usage summary. To clear up, use `clear_usage_summary()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No usage summary. Please call \"create\" first.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAIWrapper(config_list=config_list)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you give me 3 useful tips on learning Python? Keep it simple and short.\"},\n",
    "]\n",
    "client.print_usage_summary()  # print usage summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Usage summary excluding cached usage: \n",
      "Total cost: 0.00035\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00035, prompt_tokens: 25, completion_tokens: 157, total_tokens: 182\n",
      "\n",
      "All completions are non-cached: the total cost with cached completions is the same as actual cost.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Usage summary excluding cached usage: \n",
      "Total cost: 0.00035\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00035, prompt_tokens: 25, completion_tokens: 157, total_tokens: 182\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Usage summary including cached usage: \n",
      "Total cost: 0.00035\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00035, prompt_tokens: 25, completion_tokens: 157, total_tokens: 182\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The first creation\n",
    "# By default, cache_seed is set to 41 and enabled. If you don't want to use cache, set cache_seed to None.\n",
    "response = client.create(messages=messages, model=\"gpt-35-turbo-1106\", cache_seed=41)\n",
    "client.print_usage_summary()  # default to [\"actual\", \"total\"]\n",
    "client.print_usage_summary(mode=\"actual\")  # print actual usage summary\n",
    "client.print_usage_summary(mode=\"total\")  # print total usage summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_cost': 0.0003515, 'gpt-3.5-turbo-0613': {'cost': 0.0003515, 'prompt_tokens': 25, 'completion_tokens': 157, 'total_tokens': 182}}\n",
      "{'total_cost': 0.0003515, 'gpt-3.5-turbo-0613': {'cost': 0.0003515, 'prompt_tokens': 25, 'completion_tokens': 157, 'total_tokens': 182}}\n"
     ]
    }
   ],
   "source": [
    "# take out cost\n",
    "print(client.actual_usage_summary)\n",
    "print(client.total_usage_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Usage summary excluding cached usage: \n",
      "Total cost: 0.00035\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00035, prompt_tokens: 25, completion_tokens: 157, total_tokens: 182\n",
      "\n",
      "Usage summary including cached usage: \n",
      "Total cost: 0.0007\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.0007, prompt_tokens: 50, completion_tokens: 314, total_tokens: 364\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Since cache is enabled, the same completion will be returned from cache, which will not incur any actual cost.\n",
    "# So actual cost doesn't change but total cost doubles.\n",
    "response = client.create(messages=messages, model=\"gpt-35-turbo-1106\", cache_seed=41)\n",
    "client.print_usage_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No usage summary. Please call \"create\" first.\n"
     ]
    }
   ],
   "source": [
    "# clear usage summary\n",
    "client.clear_usage_summary()\n",
    "client.print_usage_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "No actual cost incurred (all completions are using cache).\n",
      "\n",
      "Usage summary including cached usage: \n",
      "Total cost: 0.00035\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00035, prompt_tokens: 25, completion_tokens: 157, total_tokens: 182\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# all completions are returned from cache, so no actual cost incurred.\n",
    "response = client.create(messages=messages, model=\"gpt-35-turbo-1106\", cache_seed=41)\n",
    "client.print_usage_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Summary for Agents\n",
    "\n",
    "- `Agent.print_usage_summary()` will print the cost summary for the agent.\n",
    "- `Agent.get_actual_usage()` and `Agent.get_total_usage()` will return the usage summary in a dict. When an agent doesn't use LLM, they will return None.\n",
    "- `Agent.reset()` will reset the usage summary.\n",
    "- `autogen.agent_utils.gather_usage_summary` will gather the usage summary for a list of agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mai_user\u001b[0m (to assistant):\n",
      "\n",
      "$x^3=125$. What is x?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ai_user):\n",
      "\n",
      "To find the value of x, we need to take the cube root of both sides of the equation:\n",
      "\n",
      "$\\sqrt[3]{x^3}=\\sqrt[3]{125}$\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "$x=5$\n",
      "\n",
      "So, x is equal to 5.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mai_user\u001b[0m (to assistant):\n",
      "\n",
      "Great job! Your answer is correct. Taking the cube root of both sides of the equation is the correct method to find the value of x. In this case, the cube root of 125 is indeed 5. Well done!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ai_user):\n",
      "\n",
      "Thank you! I'm happy to hear that the explanation was clear. If you have any more questions, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "assistant = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": None,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "ai_user_proxy = UserProxyAgent(\n",
    "    name=\"ai_user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    code_execution_config=False,\n",
    "    llm_config={\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    "    # In the system message the \"user\" always refers to the other agent.\n",
    "    system_message=\"You ask a user for help. You check the answer from the user and provide feedback.\",\n",
    ")\n",
    "assistant.reset()\n",
    "\n",
    "math_problem = \"$x^3=125$. What is x?\"\n",
    "ai_user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=math_problem,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'ai_user':\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Usage summary excluding cached usage: \n",
      "Total cost: 0.00024\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00024, prompt_tokens: 99, completion_tokens: 47, total_tokens: 146\n",
      "\n",
      "All completions are non-cached: the total cost with cached completions is the same as actual cost.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Agent 'assistant':\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Usage summary excluding cached usage: \n",
      "Total cost: 0.00041\n",
      "* Model 'gpt-3.5-turbo-0613': cost: 0.00041, prompt_tokens: 165, completion_tokens: 82, total_tokens: 247\n",
      "\n",
      "All completions are non-cached: the total cost with cached completions is the same as actual cost.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ai_user_proxy.print_usage_summary()\n",
    "print()\n",
    "assistant.print_usage_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cost incurred from agent 'user'.\n"
     ]
    }
   ],
   "source": [
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=2,\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=\"That's all. Thank you.\",\n",
    ")\n",
    "user_proxy.print_usage_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual usage summary for assistant (excluding completion from cache): {'total_cost': 0.0004115, 'gpt-3.5-turbo-0613': {'cost': 0.0004115, 'prompt_tokens': 165, 'completion_tokens': 82, 'total_tokens': 247}}\n",
      "Total usage summary for assistant (including completion from cache): {'total_cost': 0.0004115, 'gpt-3.5-turbo-0613': {'cost': 0.0004115, 'prompt_tokens': 165, 'completion_tokens': 82, 'total_tokens': 247}}\n",
      "Actual usage summary for ai_user_proxy: {'total_cost': 0.00024249999999999999, 'gpt-3.5-turbo-0613': {'cost': 0.00024249999999999999, 'prompt_tokens': 99, 'completion_tokens': 47, 'total_tokens': 146}}\n",
      "Total usage summary for ai_user_proxy: {'total_cost': 0.00024249999999999999, 'gpt-3.5-turbo-0613': {'cost': 0.00024249999999999999, 'prompt_tokens': 99, 'completion_tokens': 47, 'total_tokens': 146}}\n",
      "Actual usage summary for user_proxy: None\n",
      "Total usage summary for user_proxy: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual usage summary for assistant (excluding completion from cache):\", assistant.get_actual_usage())\n",
    "print(\"Total usage summary for assistant (including completion from cache):\", assistant.get_total_usage())\n",
    "\n",
    "print(\"Actual usage summary for ai_user_proxy:\", ai_user_proxy.get_actual_usage())\n",
    "print(\"Total usage summary for ai_user_proxy:\", ai_user_proxy.get_total_usage())\n",
    "\n",
    "print(\"Actual usage summary for user_proxy:\", user_proxy.get_actual_usage())\n",
    "print(\"Total usage summary for user_proxy:\", user_proxy.get_total_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_cost': 0.0006090000000000001,\n",
       " 'gpt-35-turbo': {'cost': 0.0006090000000000001,\n",
       "  'prompt_tokens': 242,\n",
       "  'completion_tokens': 123,\n",
       "  'total_tokens': 365}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_usage_summary, actual_usage_summary = gather_usage_summary([assistant, ai_user_proxy, user_proxy])\n",
    "total_usage_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.get_total_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
